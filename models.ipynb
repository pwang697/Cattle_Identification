{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OI8Q8iLPhDo",
        "outputId": "84c1b332-76c6-4d40-e607-0fd542d69f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan  8 16:34:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0              26W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_dir = \"/content/drive/MyDrive/Colab Notebooks/Cattle_Identification\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, notebook_dir)\n",
        "os.chdir(notebook_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxn7g3NyP36N",
        "outputId": "c0b3f006-8d18-4c2b-84b9-7f5662c37114"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)"
      ],
      "metadata": {
        "id": "qsZoHBF6az_6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "_WS9VF8ja4Xy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "XBKc83wEa7R1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, embedding_size=128):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc_embedding = nn.Linear(512 * block.expansion, embedding_size)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x_embedding = self.fc_embedding(x)\n",
        "\n",
        "        return x_embedding, 0"
      ],
      "metadata": {
        "id": "DXvwvNpgbDQP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Resnet34(embedding_size=128):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], embedding_size=embedding_size)"
      ],
      "metadata": {
        "id": "VFjZUDULRlqQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Resnet50(embedding_size=128):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], embedding_size=embedding_size)"
      ],
      "metadata": {
        "id": "jDMqWF5-dykh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Resnet101(embedding_size=128):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], embedding_size=embedding_size)"
      ],
      "metadata": {
        "id": "SVaf_XKPR0H4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "      Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3):\n",
        "        super(PatchEmbeddings, self).__init__()\n",
        "        self.patch_embeddings = nn.Conv2d(in_channels=in_channels,out_channels=patch_size*patch_size*in_channels,kernel_size=patch_size,stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, patch_size*patch_size*in_channels))\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, (img_size//patch_size)**2 + 1, patch_size*patch_size*in_channels))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x): # x:(batch_size,3,224,224)\n",
        "        B = x.shape[0] #B:batch size\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1) #cls_token:(1,1,768) -> cls_tokens:(batch_size,1,768)\n",
        "        # [B, C, H, W] -> [B, C, H*W] ->[B, H*W, C]\n",
        "        x = self.patch_embeddings(x) # x:(batch_size,768,14,14)\n",
        "        x = torch.flatten(x,start_dim=2, end_dim=3) # x:(batch_size,768,196)\n",
        "        x = x.transpose(-1, -2) # x:(batch_size,196,768)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) # x:(batch_size,197,768)\n",
        "        embeddings = x + self.position_embeddings # position_embeddings:(1,197,768)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "oEHkVnD5S8b-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self,att_heads_num=12, img_size=224, patch_size=16, in_channels=3):\n",
        "        super(Multi_Head_Attention, self).__init__()\n",
        "        self.att_heads_num = att_heads_num\n",
        "        self.att_head_size = int(patch_size*patch_size*in_channels / self.att_heads_num)  # 64\n",
        "        self.all_head_size = self.att_heads_num * self.att_head_size #768\n",
        "\n",
        "        self.query = nn.Linear(patch_size*patch_size*in_channels, self.all_head_size)\n",
        "        self.key = nn.Linear(patch_size*patch_size*in_channels, self.all_head_size)\n",
        "        self.value = nn.Linear(patch_size*patch_size*in_channels, self.all_head_size)\n",
        "\n",
        "        self.out = nn.Linear(patch_size*patch_size*in_channels, patch_size*patch_size*in_channels)\n",
        "        self.attn_dropout = nn.Dropout(0.0)\n",
        "        self.proj_dropout = nn.Dropout(0.0)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x): # (batch_size,197,768)\n",
        "        new_x_shape = \\\n",
        "            x.size()[:-1] + (self.att_heads_num, self.att_head_size)\n",
        "        x = x.view(new_x_shape) # (batch_size,197,12,64)\n",
        "        return x.permute(0, 2, 1, 3)  # (batch_size,12,197,64)\n",
        "\n",
        "    def forward(self, hidden_states): # (batch_size,197,768)\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer) # (batch_size,12,197,64)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer) # (batch_size,12,197,64)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer) # (batch_size,12,197,64)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.att_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "\n",
        "        weights =  None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() # (batch_size,197,12,64)\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "\n",
        "        return attention_output, weights"
      ],
      "metadata": {
        "id": "0rrDItlNY30e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu,\n",
        "          \"relu\": torch.nn.functional.relu}\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = nn.Linear(patch_size*patch_size*in_channels, 4*patch_size*patch_size*in_channels)\n",
        "        self.fc2 = nn.Linear(4*patch_size*patch_size*in_channels, patch_size*patch_size*in_channels)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = nn.Dropout(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "f29DKwRVhwXQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, att_heads_num=12, img_size=224, patch_size=16, in_channels=3):\n",
        "        super(Block, self).__init__()\n",
        "        self.attention_norm = nn.LayerNorm(patch_size*patch_size*in_channels, eps=1e-6)\n",
        "        self.multi_head_attn = Multi_Head_Attention(att_heads_num=att_heads_num, img_size=img_size, patch_size=patch_size, in_channels=in_channels)\n",
        "        self.ffn_norm = nn.LayerNorm(patch_size*patch_size*in_channels, eps=1e-6)\n",
        "        self.ffn = Mlp(img_size=img_size, patch_size=patch_size, in_channels=in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.multi_head_attn(x)\n",
        "        x = x + h\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights"
      ],
      "metadata": {
        "id": "gWhAg3hujpyO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_blocks=12, att_heads_num=12, img_size=224, patch_size=16, in_channels=3):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = nn.LayerNorm(patch_size*patch_size*in_channels, eps=1e-6)\n",
        "        for _ in range(num_blocks):\n",
        "            layer = Block(att_heads_num=att_heads_num, img_size=img_size, patch_size=patch_size, in_channels=in_channels)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, embedding_output):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(embedding_output)\n",
        "\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights"
      ],
      "metadata": {
        "id": "4am4E8YmkL4j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, num_blocks=12, att_heads_num=12, img_size=224, patch_size=16, in_channels=3):\n",
        "        super(ViT, self).__init__()\n",
        "        self.patchEmbeddings = PatchEmbeddings(img_size=img_size, patch_size=patch_size, in_channels=in_channels)\n",
        "        self.encoder = Encoder(num_blocks=num_blocks, att_heads_num=att_heads_num, img_size=img_size, patch_size=patch_size, in_channels=in_channels)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.patchEmbeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights"
      ],
      "metadata": {
        "id": "eWY_xkdGlPX_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTNet(nn.Module):\n",
        "    def __init__(self, num_blocks=12, att_heads_num=12, img_size=224, patch_size=16, in_channels=3, embedding_size=128):\n",
        "        super(ViTNet, self).__init__()\n",
        "        self.transformer = ViT(num_blocks=num_blocks, att_heads_num=att_heads_num, img_size=img_size, patch_size=patch_size, in_channels=in_channels)\n",
        "        self.fc_embedding = nn.Linear(patch_size*patch_size*in_channels, embedding_size)\n",
        "\n",
        "    def forward(self, x): # x:(batch_size,3,224,224)\n",
        "        x, attn_weights = self.transformer(x) # (batch_size,197,768)\n",
        "        x_embedding = self.fc_embedding(x[:, 0])\n",
        "        return x_embedding, attn_weights"
      ],
      "metadata": {
        "id": "5sFkxM5nlV7F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "\n",
        "class BalancedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    Returns batches of size n_classes * n_samples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, n_classes, n_samples):\n",
        "        self.labels = labels\n",
        "        self.labels_set = list(set(self.labels))\n",
        "        self.label_to_indices = {label: np.where(np.array(self.labels) == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.n_dataset = len(self.labels)\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < self.n_dataset:\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_dataset // self.batch_size"
      ],
      "metadata": {
        "id": "KpdaAyS9opI4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "class RandomTripletSelector():\n",
        "    \"\"\"\n",
        "    Select random negative example for each positive pair to create triplets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RandomTripletSelector, self).__init__()\n",
        "\n",
        "    def get_triplets(self, embeddings, labels):\n",
        "        labels = labels.cpu().data.numpy()\n",
        "        triplets = []\n",
        "        for label in set(labels):\n",
        "            label_mask = (labels == label)\n",
        "            label_indices = np.where(label_mask)[0]\n",
        "            if len(label_indices) < 2:\n",
        "                continue\n",
        "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
        "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
        "\n",
        "            # random choose one negative example for each positive pair\n",
        "            temp_triplets = [[anchor_positive[0], anchor_positive[1], np.random.choice(negative_indices)] for anchor_positive in anchor_positives]\n",
        "            triplets += temp_triplets\n",
        "\n",
        "        return torch.LongTensor(np.array(triplets))"
      ],
      "metadata": {
        "id": "gS5rKaRIqmbF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplets loss\n",
        "    Takes a batch of embeddings and corresponding labels.\n",
        "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
        "    triplets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, triplet_selector):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.triplet_selector = triplet_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "\n",
        "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
        "\n",
        "        if embeddings.is_cuda:\n",
        "            triplets = triplets.cuda()\n",
        "\n",
        "\n",
        "        anchor_idx= triplets[:, 0]\n",
        "        positive_idx= triplets[:, 1]\n",
        "        negative_idx= triplets[:, 2]\n",
        "\n",
        "\n",
        "        ap_distances = (embeddings[anchor_idx] - embeddings[positive_idx]).pow(2).sum(1)\n",
        "        an_distances = (embeddings[anchor_idx] - embeddings[negative_idx]).pow(2).sum(1)\n",
        "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
        "\n",
        "        return losses.mean()"
      ],
      "metadata": {
        "id": "jme4KSAbqz_H"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "class Trainer():\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 device: torch.device,\n",
        "                 criterion: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 training_DataLoader: torch.utils.data.Dataset,\n",
        "                 test_DataLoader: torch.utils.data.Dataset,\n",
        "                 epochs: int\n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.training_DataLoader = training_DataLoader\n",
        "        self.test_DataLoader = test_DataLoader\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "\n",
        "\n",
        "    def run_trainer(self):\n",
        "\n",
        "      train_loss = []\n",
        "      test_loss = []\n",
        "\n",
        "      for epoch in tqdm(range(self.epochs)):\n",
        "          self.model.train()  # train mode\n",
        "\n",
        "          train_losses=[]\n",
        "          for batch in self.training_DataLoader:\n",
        "\n",
        "              x,y=batch\n",
        "              input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "              self.optimizer.zero_grad()  # zerograd the parameters\n",
        "              out, _ = self.model(input)  # one forward pass\n",
        "              loss = self.criterion(out, target)  # calculate loss\n",
        "\n",
        "              loss_value = loss.item()\n",
        "              train_losses.append(loss_value)\n",
        "\n",
        "              loss.backward()  # one backward pass\n",
        "              self.optimizer.step()  # update the parameters\n",
        "\n",
        "\n",
        "\n",
        "          self.model.eval()  # evaluation mode\n",
        "          test_losses = []  # accumulate the losses here\n",
        "\n",
        "          for batch in self.test_DataLoader:\n",
        "\n",
        "              x,y=batch\n",
        "              input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  out, _ = self.model(input)   # one forward pass\n",
        "                  loss = self.criterion(out, target) # calculate loss\n",
        "\n",
        "                  loss_value = loss.item()\n",
        "                  test_losses.append(loss_value)\n",
        "\n",
        "          mean_train_loss = np.mean(train_losses)\n",
        "          mean_test_loss = np.mean(test_losses)\n",
        "          train_loss.append(mean_train_loss)\n",
        "          test_loss.append(mean_test_loss)\n",
        "\n",
        "          # print the results\n",
        "          print(f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',end=' ')\n",
        "          print(f'TRAIN-LOSS: {mean_train_loss:.4f}',end=' ')\n",
        "          print(f'TEST-LOSS: {mean_test_loss:.4f}',end='\\n')\n",
        "\n",
        "      plt.figure(figsize=(8,6))\n",
        "      plt.title('Loss curve')\n",
        "      plt.plot(range(self.epochs), train_loss, label=\"train_loss\")\n",
        "      plt.plot(range(self.epochs), test_loss, label=\"test_loss\")\n",
        "      plt.legend()\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show"
      ],
      "metadata": {
        "id": "NKHlw2acq5TN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_generator(images, labels, n_queries, n_candidates):\n",
        "  import numpy as np\n",
        "  import random\n",
        "  labels = labels.cpu().data.numpy()\n",
        "  queries = []\n",
        "  for label in set(labels):\n",
        "    label_mask = (labels == label)\n",
        "    label_indices = np.where(label_mask)[0]\n",
        "    if len(label_indices) < 2:\n",
        "        continue\n",
        "    negative_indices = np.where(np.logical_not(label_mask))[0]\n",
        "    anchor_positives = list(combinations(label_indices, 2))\n",
        "    for anchor_positive in anchor_positives:\n",
        "      anchor = anchor_positive[0]\n",
        "      positive = anchor_positive[1]\n",
        "      negatives = np.random.choice(negative_indices, size=n_candidates-1, replace=False)\n",
        "      truth = np.random.randint(n_candidates)\n",
        "      candidates = np.insert(negatives, truth, positive)\n",
        "      queries.append([anchor, candidates, truth])\n",
        "  sampled_queries = random.sample(queries, n_queries)\n",
        "  anchor_images, candidates_images, truth = [],[],[]\n",
        "  for sampled_query in sampled_queries:\n",
        "    anchor_images.append(images[sampled_query[0]])\n",
        "    temp_candidate_images = []\n",
        "    for index in sampled_query[1]:\n",
        "      temp_candidate_images.append(images[index])\n",
        "    tensor_candidate_images = torch.stack(temp_candidate_images, 0)\n",
        "    candidates_images.append(tensor_candidate_images)\n",
        "    truth.append(sampled_query[2])\n",
        "  return torch.stack(anchor_images, 0), torch.stack(candidates_images, 0), truth"
      ],
      "metadata": {
        "id": "31OjjvvMVaKI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to determine performance of model\n",
        "def query_performance(model, queries, targets, truth, top=1):\n",
        "  from scipy.spatial.distance import cdist\n",
        "  assert top >= 1\n",
        "  cnt = 0\n",
        "  for i in range(len(truth)):\n",
        "      q = queries[i][None].float().cuda()\n",
        "      t = targets[i].float().cuda()\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "          emb_q = model(q)[0].cpu().numpy()\n",
        "          emb_t = model(t)[0].cpu().numpy()\n",
        "\n",
        "          dists = cdist(emb_q, emb_t)\n",
        "\n",
        "          if top == 1:\n",
        "              pred = np.argmin(dists)\n",
        "\n",
        "              if pred == truth[i]:\n",
        "                  cnt += 1\n",
        "\n",
        "          else:\n",
        "              pred = np.argsort(dists)\n",
        "              if truth[i] in pred[0,:top].tolist():\n",
        "                  cnt+=1\n",
        "  return (cnt/len(truth))"
      ],
      "metadata": {
        "id": "mjJm8qebVSqG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_performance(model, images, labels, n_queries, n_candidates, iters):\n",
        "  tops = list(range(1,n_candidates))\n",
        "  acc = np.zeros(n_candidates - 1)\n",
        "  for _ in range(iters):\n",
        "    queries_tensor, targets_tensor, truth = query_generator(images, labels, n_queries, n_candidates)\n",
        "    for top in tops:\n",
        "      acc[top-1] += query_performance(model, queries_tensor, targets_tensor, truth, top)\n",
        "  results = np.divide(acc, iters)\n",
        "  for top in tops:\n",
        "    print(f'top{top} acc: {results[top-1]:.4f}',end='\\n')"
      ],
      "metadata": {
        "id": "HNiKpzQ0VM6V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_label_split(npz):\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  X_array = npz['images']\n",
        "  y_array = npz['labels']\n",
        "  state = np.random.get_state()\n",
        "  np.random.shuffle(X_array)\n",
        "  np.random.set_state(state)\n",
        "  np.random.shuffle(y_array)\n",
        "  return torch.tensor(np.transpose(X_array, (0,3,1,2))).float(), torch.tensor(y_array).float()"
      ],
      "metadata": {
        "id": "-oWBAIlyrZb8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = feature_label_split(np.load(notebook_dir + '/datasets/70-30/resize/train.npz'))\n",
        "X_test, y_test = feature_label_split(np.load(notebook_dir + '/datasets/70-30/resize/test.npz'))\n",
        "X_eval, y_eval = feature_label_split(np.load(notebook_dir + '/datasets/70-30/resize/eval.npz'))\n",
        "print(f\"Image train shape: {X_train.shape}\")\n",
        "print(f\"Label train shape: {y_train.shape}\")\n",
        "print(f\"Image test shape: {X_test.shape}\")\n",
        "print(f\"Label test shape: {y_test.shape}\")\n",
        "print(f\"Image eval shape: {X_eval.shape}\")\n",
        "print(f\"Label eval shape: {y_eval.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZb-dppursY2",
        "outputId": "b5ded9fb-9476-4c87-b0d9-8a25a15b95e3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image train shape: torch.Size([587, 3, 224, 224])\n",
            "Label train shape: torch.Size([587])\n",
            "Image test shape: torch.Size([126, 3, 224, 224])\n",
            "Label test shape: torch.Size([126])\n",
            "Image eval shape: torch.Size([295, 3, 224, 224])\n",
            "Label eval shape: torch.Size([295])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "Tt0Okg8kr1mw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "learning_rate = 1e-4\n",
        "epochs = 50\n",
        "n_classes = 32\n",
        "n_samples = 2\n",
        "margin = 0.4\n",
        "embedding_size = 256\n",
        "\n",
        "# device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "\n",
        "# model\n",
        "model = ViTNet(embedding_size=embedding_size).to(device)\n",
        "# model = Resnet34(embedding_size=embedding_size).to(device)\n",
        "# model = Resnet50(embedding_size=embedding_size).to(device)\n",
        "# model = Resnet101(embedding_size=embedding_size).to(device)\n",
        "\n",
        "# dataloaders\n",
        "train_batch_sampler = BalancedBatchSampler(y_train.tolist(), n_classes=n_classes, n_samples=n_samples)\n",
        "test_batch_sampler = BalancedBatchSampler(y_test.tolist(), n_classes=n_classes, n_samples=n_samples)\n",
        "\n",
        "triplets_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler)\n",
        "triplets_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler)\n",
        "\n",
        "\n",
        "# criterion\n",
        "criterion = TripletLoss(margin,  RandomTripletSelector())\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(model=model,\n",
        "                  device=device,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer,\n",
        "                  training_DataLoader=triplets_train_loader,\n",
        "                  test_DataLoader=triplets_test_loader,\n",
        "                  epochs=epochs)\n",
        "\n",
        "# start training\n",
        "trainer.run_trainer()\n",
        "\n",
        "#evaluate model\n",
        "eval_performance(model, X_eval, y_eval, 50, 5, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EYK8mV9sEtk",
        "outputId": "51cba044-9197-43dc-a105-dcc5b4b1592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/50 [00:02<02:04,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 01/50 TRAIN-LOSS: 0.1310 TEST-LOSS: 0.0634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/50 [00:04<01:57,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 02/50 TRAIN-LOSS: 0.1108 TEST-LOSS: 0.1410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/50 [00:07<01:53,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 03/50 TRAIN-LOSS: 0.0979 TEST-LOSS: 0.0844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/50 [00:09<01:50,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 04/50 TRAIN-LOSS: 0.0773 TEST-LOSS: 0.1049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/50 [00:12<01:47,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 05/50 TRAIN-LOSS: 0.0786 TEST-LOSS: 0.0401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/50 [00:14<01:44,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 06/50 TRAIN-LOSS: 0.0719 TEST-LOSS: 0.1260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/50 [00:16<01:42,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 07/50 TRAIN-LOSS: 0.0767 TEST-LOSS: 0.1309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 8/50 [00:19<01:40,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 08/50 TRAIN-LOSS: 0.0642 TEST-LOSS: 0.0175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 9/50 [00:21<01:37,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 09/50 TRAIN-LOSS: 0.0481 TEST-LOSS: 0.1649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [00:23<01:35,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 10/50 TRAIN-LOSS: 0.0491 TEST-LOSS: 0.0676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 11/50 [00:26<01:32,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 11/50 TRAIN-LOSS: 0.0673 TEST-LOSS: 0.0713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 12/50 [00:28<01:30,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 12/50 TRAIN-LOSS: 0.0646 TEST-LOSS: 0.0325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 13/50 [00:31<01:28,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 13/50 TRAIN-LOSS: 0.0758 TEST-LOSS: 0.0080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 14/50 [00:33<01:25,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 14/50 TRAIN-LOSS: 0.0441 TEST-LOSS: 0.1439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 15/50 [00:35<01:23,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 15/50 TRAIN-LOSS: 0.0609 TEST-LOSS: 0.1187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 16/50 [00:38<01:21,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 16/50 TRAIN-LOSS: 0.0770 TEST-LOSS: 0.0719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 90-10\n",
        "x_axis = ['top1','top2','top3','top4']\n",
        "y_ViT = [0.9147, 0.9887, 0.9987, 1.0000]\n",
        "y_Resnet34 = [0.8913, 0.9753, 0.9947, 1.0000]\n",
        "y_Resnet50 = [0.7600, 0.8967, 0.9687, 0.9933]\n",
        "y_Resnet101 = [0.6493, 0.8487, 0.9513, 0.9920]"
      ],
      "metadata": {
        "id": "KUpcucQwkfsu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 70-30\n",
        "x_axis = ['top1','top2','top3','top4']\n",
        "y_ViT = []\n",
        "y_Resnet34 = [0.8587, 0.9673, 0.9927, 1.0000]\n",
        "y_Resnet50 = [0.6973, 0.8960, 0.9680, 0.9953]\n",
        "y_Resnet101 = [0.6473, 0.8680, 0.9560, 0.9920]"
      ],
      "metadata": {
        "id": "ANFj6eQ1sZ5L"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 50-50\n",
        "x_axis = ['top1','top2','top3','top4']\n",
        "y_ViT = []\n",
        "y_Resnet34 = []\n",
        "y_Resnet50 = []\n",
        "y_Resnet101 = []"
      ],
      "metadata": {
        "id": "6Usap2IS8P34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30-70\n",
        "x_axis = ['top1','top2','top3','top4']\n",
        "y_ViT = []\n",
        "y_Resnet34 = []\n",
        "y_Resnet50 = []\n",
        "y_Resnet101 = []"
      ],
      "metadata": {
        "id": "FBDNQR9H8Qnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10-90\n",
        "x_axis = ['top1','top2','top3','top4']\n",
        "y_ViT = []\n",
        "y_Resnet34 = []\n",
        "y_Resnet50 = []\n",
        "y_Resnet101 = []"
      ],
      "metadata": {
        "id": "qGmucX8r8RHy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}